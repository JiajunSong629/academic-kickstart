---
title: "Prostate Cancer example"
linktitle: "Prostate Cancer example"
date: "2020-05-11"
output:
  blogdown::html_page:
    toc: true
menu:
  esl:
    parent: Chapter 3
    weight: 2
type: docs
weight: 1
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Load library and data

```{r}
library(tidyverse)
library(knitr)
library(kableExtra)
library(xtable)

prostate_raw <- read.table(here::here("static", "data", "prostate.data"))

# to replicate the results, however would violate the idea
# of train/test split
prostate <- prostate_raw
prostate[, 1:8] <- scale(prostate[, 1:8])
#summary(prostate)
```

## Duplicate of Table 3.1

```{r}
Training <- subset(prostate, train)
Testing <- subset(prostate, !train)

p <- 8; nrow <- nrow(Training)

Xtrain <- as.matrix(Training[, 1:p])
ytrain <- as.matrix(Training[, p+1]); colnames(ytrain) <- colnames(Training)[p+1]
Xtest <- as.matrix(Testing[, 1:p])
ytest <- as.matrix(Testing[, p+1]); colnames(ytest) <- colnames(Testing)[p+1]

kable(round(cor(Xtrain), digits=3),
      caption = "Correlation matrix (after global scaling)")
```


## Duplicate of Table 3.2

```{r}
Xp <- cbind(matrix(1, nrow, 1), Xtrain)
beta_hat <- solve(t(Xp) %*% Xp) %*% t(Xp) %*% ytrain

yfit <- Xp %*% beta_hat
sigma2_hat <- sum((yfit - ytrain)^2) / (nrow - p - 1)
se <- sqrt(sigma2_hat * diag(solve(t(Xp) %*% Xp)))

lm_summary <- data.frame(
  cbind(beta_hat, se, beta_hat/se),
  row.names = c("Intercept", names(se)[-1])
)
colnames(lm_summary) <- c("Coefficient", "Std. Err", "Z Score")

kable(lm_summary, digits = 2,
      caption = "Linear model fit summary")
```


## Comments

F-statistic on non-signifcant terms, `age`, `lcp`, `gleason`, `pgg45`.

```{r}
df <- as.data.frame(cbind(Xtrain, ytrain))

full_lm <- lm(lpsa ~ ., data = df)
sub_lm <- lm(lpsa ~ . -age-lcp-gleason-pgg45, data = df)

full_rss <- sum(full_lm$residuals^2)
sub_rss <- sum(sub_lm$residuals^2)

Fstat <- ((sub_rss - full_rss) / (sub_lm$df - full_lm$df)) / (full_rss / full_lm$df)
pval <- pf(Fstat, df1 = sub_lm$df - full_lm$df, df2 = full_lm$df, lower.tail = F)

ftest <- data.frame(
  matrix(c(Fstat, sub_lm$df-full_lm$df, full_lm$df, pval), nrow = 1)
)

colnames(ftest) <- c("F-stats", "df1", "df2", "p-value")

kable(ftest, digits = 2,
      caption = "F-test on nonsignifcant terms")
```



```{r}
ypred_test <- predict(full_lm, as.data.frame(Xtest))
mean_pred_err <- mean((ypred_test - as.matrix(ytest))^2)
base_err <- mean((as.matrix(ytest) - mean(ytrain))^2)

data.frame(matrix(c(mean_pred_err, base_err), nrow = 1)) %>%
  `colnames<-`(c("Mean prediction error", "Base error")) %>%
  kable(digits = 2, caption = "Linear fit reduces the error by 50%")
```


## Duplicate of Figure 3.5

```{r}
# get k = 0 (no features) model
obj <- lm(lpsa ~ +1, data=df)
xPlot <- c(0); yPlot <- c(sum(obj$residual^2))

for (k in 1:p) {
  allPosSubsetsSizeK = combn(p,k)
  numOfSubsets = dim(allPosSubsetsSizeK)[2]
  
  for (si in 1:numOfSubsets) {
    featIndices = allPosSubsetsSizeK[, si]
    featNames = as.vector(names(df))[featIndices]
    
    formula = "lpsa ~"
    for (ki in 1:k) {
      if (ki == 1) {
        formula = paste(formula, featNames[ki], sep = " ")
      } else {
        formula = paste(formula, featNames[ki], sep = "+")
      }
    }
    
    obj <- lm(formula, data = df)
    xPlot <- c(xPlot, k); yPlot <- c(yPlot, sum(obj$residuals^2))
  }
}

xMinPlot = xPlot[1]; yMinPlot = yPlot[1]
for (ki in 1:p) {
  rssmin = min(yPlot[xPlot == ki])
  xMinPlot = c(xMinPlot, ki)
  yMinPlot = c(yMinPlot, rssmin)
}

ggplot() +
  geom_point(aes(x = xPlot, y = yPlot), color = "gray50") +
  geom_point(aes(x = xMinPlot, y = yMinPlot), color = "red", size = 2) +
  geom_line(aes(x = xMinPlot, y = yMinPlot), color = "red") +
  scale_x_continuous(breaks = 0:8) +
  #scale_y_continuous(breaks = seq(0, 100, 20)) +
  lims(y = c(-5, 100)) +
  labs(x = "Subset size k", y = "Residual Sum-of-Squares") +
  theme_classic()

```


## Duplicate of Figure 3.7

```{r}
# cross-validation helper function
cv_all_subsets <- function(k, df, numberOfCV=10) {
  nSamples = dim(df)[1]; p = dim(df)[2]-1
  responseName = names(df)[p+1]
  
  if (k == 0) {
    form = paste(responseName, "~ +1")
    bestErrors = cvLM(form, df, numberOfCV)
    bestFeaturesIndices = NULL
    bestFormula = form
  } else {
    allPosSubsetsSizeK = combn(p, k)
    numOfSubsets = dim(allPosSubsetsSizeK)[2]
    
    for (si in 1:numOfSubsets) {
      featuresIndices = allPosSubsetsSizeK[, si]
      featuresNames = as.vector(names(df))[featuresIndices]
      
      form = paste(responseName, " ~ ")
      for (ki in 1:k) {
        if (ki == 1) {
          form = paste(form, featuresNames[ki], sep = " ")
        } else {
          form = paste(form, featuresNames[ki], sep = "+")
        }
      }
      
      expectedPredictionErrors = cvLM(form, df, numberOfCV)
      
      if (si == 1) {
        bestErrors = expectedPredictionErrors
        bestFeaturesIndices = featuresIndices
        bestFormula = form
      } else {
        if (mean(expectedPredictionErrors) < mean(bestErrors)) {
          bestErrors = expectedPredictionErrors
          bestFeaturesIndices = featuresIndices
          bestFormula = form
        }
      }
    }
  }
  
  res = list(bestErrors, bestFeaturesIndices, bestFormula)
  return (res)
}

```


```{r}
cvLM <- function(form, df, numberOfCV=10) {
  nSamples = dim(df)[1]; p = dim(df)[2]-1
  nCVTest = round(nSamples / numberOfCV)
  
  # test and train indices
  for (cvi in 1:numberOfCV) {
    testinds = (cvi-1)*nCVTest + 1:nCVTest
    testinds = intersect(testinds, 1:nSamples)
    traininds = setdiff(1:nSamples, testinds)
    
    dcv = df[traininds, ]
    obj <- lm(formula = form, data = dcv)
    ypred_test <- predict(obj, newdata = df[testinds, 1:p], interval = "prediction")[, 1]
    response <- df[testinds, p+1]
    
    if (cvi == 1) {
      ypred = c(ypred_test)
      y = c(response)
    } else{
      ypred = c(ypred, ypred_test)
      y = c(y, response)
    }
  }
  
  return ((ypred - y)^2)
}
```



```{r}
# All subsets
for (k in 0:p) {
  res = cv_all_subsets(k, df, numberOfCV = 10)
  
  if (k == 0) {
    complexityParam = c(k)
    cvResults = res[[1]]
    bestFormula = res[[3]]
  } else {
    complexityParam = c(complexityParam, k)
    cvResults = cbind(cvResults, res[[1]])
    bestFormula = c(bestFormula, res[[3]])
  }
}

cvResults <- data.frame(cvResults) %>%
  `colnames<-`(paste0("subset", 0:8))

cvErrors <- apply(cvResults, 2, mean)
cvSd <- sqrt( apply(cvResults,2,var)/dim(cvResults)[1] ) 

ggplot() +
  geom_errorbar(aes(x = 0:8, y = cvErrors, ymin = cvErrors-cvSd, ymax = cvErrors+cvSd),
                  color = "#70B4E0", width = 0.2) +
  geom_line(aes(x = 0:8, y = cvErrors), color = "#D6B62A") +
  geom_point(aes(x = 0:8, y = cvErrors), color = "#D6B62A") +
  scale_y_continuous(breaks = seq(0.6, 1.8, 0.2)) +
  labs(x = "Subset size", y = "CV Error") +
  theme_classic()

#lm(bestFormula[3], data=df)$coef
```















